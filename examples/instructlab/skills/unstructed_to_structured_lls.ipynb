{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstructLab Skills Synthetic Data Generation\n",
    "\n",
    "![InstructLab Banner](../../../assets/imgs/instructlab-banner.png)\n",
    "\n",
    "This notebook demonstrates how to customize language models by generating training data for specific skills, following the methodology outlined in the LAB (Large-scale Alignment for Chatbots) framework [[paper link](https://arxiv.org/pdf/2403.01081)].\n",
    "\n",
    "### Customizing Model Behavior\n",
    "\n",
    "The LAB framework enables us to shape how a model responds to various tasks by training it on carefully crafted examples. Want your model to write emails in your company's tone? Need it to follow specific formatting guidelines? This customization is achieved through what the paper defines as compositional skills.\n",
    "\n",
    "Compositional skills are tasks that combine different abilities to handle complex queries. For example, if you want your model to write company emails about quarterly performance, it needs to:\n",
    "- Understand financial concepts\n",
    "- Perform basic arithmetic\n",
    "- Write in your preferred communication style\n",
    "- Follow your organization's email format\n",
    "\n",
    "### Demo Overview\n",
    "\n",
    "This notebook will show you how to:\n",
    "1. Set up a teacher model for generating training data\n",
    "2. Create examples that reflect your preferred style and approach\n",
    "3. Generate Synthetic Data\n",
    "4. Validate that the generated data matches your requirements\n",
    "\n",
    "The end goal is to create training data that will help align the model with your specific needs, whether that's matching your company's communication style, following particular protocols, or handling specialized tasks in your preferred way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßë‚Äçüè´ Step 1: Serving Teacher Model using Llama Stack\n",
    "\n",
    "The [llama-stack-openai-client library](https://github.com/instructlab/lls-openai-client) provides an OpenAI-compatible interface to Llama Stack's inference API. This allows us to use familiar OpenAI-style code while under the hood, it delegates all inference calls to the LlamaStack API.\n",
    "\n",
    "First, install the required packages:\n",
    "```bash\n",
    "pip install llama-stack aiosqlite autoevals datasets faiss-cpu mcp opentelemetry-exporter-otlp-proto-http opentelemetry-sdk\n",
    "pip install git+https://github.com/instructlab/lls-openai-client.git\n",
    "```\n",
    "\n",
    "For this demo we will use Mixtral-8x7B-Instruct-v0.1 as our teacher model\n",
    "\n",
    "Launch the vLLM server with the following command:\n",
    "```bash\n",
    "vllm serve mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 2\n",
    "```\n",
    "\n",
    "This will host the model endpoint with default address being `http://localhost:8000`\n",
    "\n",
    "\n",
    "#### Requirements & Considerations\n",
    "- Sufficient GPU memory \n",
    "- Adjust tensor-parallel-size according to available GPUs\n",
    "- Initial model loading may take several minutes\n",
    "\n",
    "#### Let's test the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.conda/envs/lls/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: `bwrap` is not available. Code interpreter tool will not work correctly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">remote-vllm</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mremote-vllm\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "container_image: null\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "image_name: remote-vllm\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/lab/.llama/distributions/remote-vllm/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: mistralai/Mixtral-8x7B-Instruct-v0.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: vllm-inference\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: sentence-transformers\n",
       "  provider_model_id: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/lab/.llama/distributions/remote-vllm/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/lab/.llama/distributions/remote-vllm/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">huggingface_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/lab/.llama/distributions/remote-vllm/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">localfs_datasetio.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/lab/.llama/distributions/remote-vllm/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">meta_reference_eval.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      api_token: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_tokens: <span style=\"color: #008000; text-decoration-color: #008000\">'4096'</span>\n",
       "      tls_verify: <span style=\"color: #008000; text-decoration-color: #008000\">'true'</span>\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/v1</span>\n",
       "    provider_id: vllm-inference\n",
       "    provider_type: remote::vllm\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: sentence-transformers\n",
       "    provider_type: inline::sentence-transformers\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: <span style=\"font-weight: bold\">[]</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: <span style=\"color: #008000; text-decoration-color: #008000\">\"\\u200B\"</span>\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/lab/.llama/distributions/remote-vllm/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/home/lab/.llama/distributions/remote-vllm/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  auth: null\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: <span style=\"font-weight: bold\">[]</span>\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "container_image: null\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "image_name: remote-vllm\n",
       "logging: null\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/home/lab/.llama/distributions/remote-vllm/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: mistralai/Mixtral-8x7B-Instruct-v0.\u001b[1;36m1\u001b[0m\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: vllm-inference\n",
       "  provider_model_id: null\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: sentence-transformers\n",
       "  provider_model_id: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/home/lab/.llama/distributions/remote-vllm/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/lab/.llama/distributions/remote-vllm/\u001b[0m\u001b[95mhuggingface_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/lab/.llama/distributions/remote-vllm/\u001b[0m\u001b[95mlocalfs_datasetio.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/lab/.llama/distributions/remote-vllm/\u001b[0m\u001b[95mmeta_reference_eval.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      api_token: \u001b[32m'********'\u001b[0m\n",
       "      max_tokens: \u001b[32m'4096'\u001b[0m\n",
       "      tls_verify: \u001b[32m'true'\u001b[0m\n",
       "      url: \u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
       "    provider_id: vllm-inference\n",
       "    provider_type: remote::vllm\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: sentence-transformers\n",
       "    provider_type: inline::sentence-transformers\n",
       "  safety:\n",
       "  - config:\n",
       "      excluded_categories: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: \u001b[32m\"\\u200B\"\u001b[0m\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/home/lab/.llama/distributions/remote-vllm/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: model-context-protocol\n",
       "    provider_type: remote::model-context-protocol\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: wolfram-alpha\n",
       "    provider_type: remote::wolfram-alpha\n",
       "  vector_io:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/home/lab/.llama/distributions/remote-vllm/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  auth: null\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: rag-runtime\n",
       "  toolgroup_id: builtin::rag\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: wolfram-alpha\n",
       "  toolgroup_id: builtin::wolfram_alpha\n",
       "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "from lls_openai_client.client_adapter import OpenAIClientAdapter\n",
    "\n",
    "\n",
    "# Set VLLM_URL environment variable\n",
    "os.environ[\"VLLM_URL\"] = \"http://localhost:8000/v1\"\n",
    "os.environ[\"INFERENCE_MODEL\"] = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Create and initialize our Llama Stack client\n",
    "lls_client = LlamaStackAsLibraryClient(\"remote-vllm\")\n",
    "lls_client.initialize()\n",
    "\n",
    "# Wrap the Llama Stack client in an OpenAI client API\n",
    "client = OpenAIClientAdapter(lls_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful! mistralai/Mixtral-8x7B-Instruct-v0.1:  Hello! It's nice to meet you.\n"
     ]
    }
   ],
   "source": [
    "teacher_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Test the connection with a simple completion\n",
    "response = client.chat.completions.create(\n",
    "    model=teacher_model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    temperature=0.0,\n",
    "    max_tokens=10\n",
    ")\n",
    "completion = response.choices[0].message.content\n",
    "\n",
    "print(f\"Connection successful! {teacher_model}: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Step 2: Provide Custom Examples\n",
    "\n",
    "\n",
    "#### Usecase: Teaching a Language Model the Skill: Unstructured Text ‚Üí Markdown Table\n",
    "\n",
    "Company X receives large volumes of user feedback through support emails, in-app surveys, and app store reviews. These messages often contain valuable product insights, but the content is unstructured and difficult to analyze at scale.\n",
    "\n",
    "To streamline internal workflows, an AI team at Company X wants to teach a language model how to convert raw user feedback into structured markdown tables. These tables summarize key topics, user sentiment, and issues in a format that‚Äôs easy to scan, report, or push into dashboards and tracking systems.\n",
    "\n",
    "We can do this using InstructLab!\n",
    "\n",
    "#### üßæ Example Input and Output\n",
    "\n",
    "üì• Input (Unstructured Feedback)\n",
    "```\n",
    "Hey team ‚Äî I‚Äôve been using the new update for about a week now.\n",
    "\n",
    "Couple of things:\n",
    "- The dark mode is awesome, great job!\n",
    "- But the loading time after login feels slower than before. Not a deal breaker but noticeable.\n",
    "- I also noticed that the calendar widget doesn‚Äôt update properly if I change time zones.\n",
    "\n",
    "Overall, I love where this is going. Just needs a few tweaks.\n",
    "```\n",
    "üì§ Output (Markdown Table)\n",
    "\n",
    "| Feature           | Feedback                                                               | Sentiment |\n",
    "|------------------|------------------------------------------------------------------------|-----------|\n",
    "| Dark Mode        | Works well, user is satisfied.                                          | Positive  |\n",
    "| Login Performance| Loading time after login is slower than previous version.               | Negative  |\n",
    "| Calendar Widget  | Doesn't update correctly when time zones change.                        | Negative  |\n",
    "| Overall          | User is happy with the direction of the product, but suggests tweaks.   | Positive  |\n",
    "\n",
    "#### Instructlab Grounded Skills Generation Pipeline \n",
    "\n",
    "Now that we have laid out our usecase, lets dive into the skills generation pipeline proposed by LAB \n",
    "You can refer to the flow details and block config from this yaml (src/instructlab/sdg/flows/generation/skills/simple_grounded_skill.yaml)\n",
    "\n",
    "InstructLab uses a multi-step process of generation and evaluation to generate synthetic data. For grounded skills it looks like this: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "    <img src=\"../../../assets/imgs/IL_skills_pipeline.png\" alt=\"Skills Pipeline\" width=\"250\">\n",
    "  </td>\n",
    "  <td>\n",
    "    <ul>\n",
    "      <li>\n",
    "        <strong>Context Generation (<code>gen_contexts</code>)</strong><br>\n",
    "        Generates diverse, relevant contexts for the skill<br>\n",
    "        Produces 10 unique contexts per run<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Question Generation & Validation</strong><br>\n",
    "        <code>gen_grounded_questions</code>: Creates 3 questions per context<br>\n",
    "        <code>eval_grounded_questions</code>: Evaluates question quality<br>\n",
    "        <code>filter_grounded_questions</code>: Keeps only perfect scores (1.0)<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Response Generation & Quality Control</strong><br>\n",
    "        <code>gen_grounded_responses</code>: Generates appropriate responses<br>\n",
    "        <code>evaluate_grounded_qa_pair</code>: Scores Q&A pair quality<br>\n",
    "        <code>filter_grounded_qa_pair</code>: Retains high-quality pairs (score ‚â• 2.0)<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Final Processing</strong><br>\n",
    "        <code>combine_question_and_context</code>: Merges context with questions for complete examples<br><br>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Data with Examples\n",
    "Now that we've seen how LAB generates skill-specific data, let's walk through how to use it for our own use case.\n",
    "\n",
    "As outlined in the LAB paper, the first step is to provide a small number of **seed examples** (typically 5) to bootstrap the skill. These examples are passed into the generation pipeline as input and are stored in a `.jsonl` file.\n",
    "\n",
    "For this demo, we‚Äôll use the pre-populated seed file located at: [mdtable_seeds.jsonl](examples/instructlab/skills/sample_data/mdtable_seeds.jsonl)\n",
    "\n",
    "Lets open the file and explore a row: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 5 examples [00:00, 976.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'task_description': 'Convert the following unstructured user feedback into a structured markdown table.',\n",
       " 'seed_context': \"Been using the new dashboard for a few days. It's way faster than the previous one, really appreciate the snappy filters. But export to CSV seems broken ‚Äî nothing happens when I click it. Also, dark mode resets every time I log in.\",\n",
       " 'seed_question': 'I would like to convert the above feedback into a markdown table with columns for Feature, Feedback and Sentiment.',\n",
       " 'seed_response': \"| Feature           | Feedback                                                           | Sentiment |\\n|------------------|--------------------------------------------------------------------|-----------|\\n| Dashboard        | Much faster than previous version, filters are responsive.         | Positive  |\\n| Export to CSV    | Clicking the export button doesn't trigger a download.             | Negative  |\\n| Dark Mode        | Resets to light mode on login.                                     | Negative  |\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the seed dataset\n",
    "seed_data = load_dataset(\"json\", data_files=\"sample_data/mdtable_seeds.jsonl\", split=\"train\")\n",
    "\n",
    "# Display the first example\n",
    "seed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Generate Synthetic Data\n",
    "\n",
    "Now that we have our seed data ready, we can use LAB‚Äôs Skill Data Generator to create **high-quality synthetic training examples** for our custom skill.\n",
    "\n",
    "This step leverages a predefined **flow configuration** that encodes how seed examples are expanded ‚Äî by generating new contexts, questions, and responses, and filtering them for quality.\n",
    "\n",
    "In this demo, we'll use the `synth_grounded_skills.yaml` flow, which follows LAB's grounded generation pattern (context ‚Üí question ‚Üí response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdg_hub.flow import Flow\n",
    "from sdg_hub.pipeline import Pipeline\n",
    "from sdg_hub.sdg import SDG\n",
    "\n",
    "# Path to the skill generation flow configuration\n",
    "flow_path = \"../../../src/sdg_hub/flows/generation/skills/synth_grounded_skills.yaml\"\n",
    "\n",
    "# Load the flow\n",
    "flow = Flow(client).get_flow_from_file(flow_path)\n",
    "\n",
    "# Initialize the synthetic data generator\n",
    "generator = SDG(\n",
    "    [Pipeline(flow)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the generator is ready to run the full pipeline ‚Äî including context generation, question/response generation, evaluation, and filtering ‚Äî to produce a synthetic dataset that can be used for fine-tuning or skill bootstrapping.\n",
    "\n",
    "In the next step, we‚Äôll run this pipeline and inspect the generated outputs. (This should take about a minute or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 260/260 [00:00<00:00, 15978.30 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 260/260 [00:00<00:00, 68590.42 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 260/260 [00:00<00:00, 47451.01 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 14980.17 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 63295.25 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 40372.98 examples/s]\n",
      "Map (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205/205 [00:00<00:00, 1378.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "generated_data = generator.generate(seed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Explore and Validate the Synthetically Generated Data\n",
    "\n",
    "Once the skill generation pipeline has been executed, the output is a set of **synthetically generated examples** ‚Äî new context-question-response triples that follow the same structure as the seed data but are expanded and refined by the teacher model.\n",
    "\n",
    "Below is an example of one generated entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "rand_idx = random.choice(range(len(generated_data)))\n",
    "generated_data[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user provided the following unstructured feedback: \"The app's interface is intuitive and user-friendly. However, I faced some issues with image loading. Occasionally, images take too long to load, or they don't load at all. Additionally, I would appreciate more customization options for the home screen. I find myself wanting to rearrange the icons or add more widgets.\"\n",
      "\n",
      "Can you help me convert the above feedback into a markdown table with columns for Feature, Feedback, and Sentiment, where the feature for the first feedback would be \"App's Interface\", the feedback would be \"The app's interface is intuitive and user-friendly\", and the sentiment would be \"Positive\"?\n"
     ]
    }
   ],
   "source": [
    "print(generated_data[rand_idx]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Feature                | Feedback                                                                  | Sentiment |\n",
      "|------------------------|---------------------------------------------------------------------------|-----------|\n",
      "| App's Interface        | The app's interface is intuitive and user-friendly.                       | Positive  |\n",
      "| Image Loading          | Occasionally, images take too long to load or don't load at all.          | Negative  |\n",
      "| Customization Options  | Desire more customization options for the home screen, such as rearranging icons or adding widgets. | Neutral   |\n"
     ]
    }
   ],
   "source": [
    "print(generated_data[rand_idx]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to teach a custom skill to a language model using the InstructLab Skill Data Generator (SDG). Starting from a small set of seed examples, we walked through the full synthetic data generation pipeline ‚Äî including context creation, question generation, response synthesis, evaluation, and filtering.\n",
    "\n",
    "We explored a real-world use case: **transforming unstructured user feedback into structured markdown tables**, and showed how the LAB framework can automate the generation of high-quality, instructional training data at scale.\n",
    "\n",
    "This approach is especially powerful for procedural or domain-specific tasks where labeled data is scarce but consistent task logic can be modeled. With just a few carefully curated seed examples, you can unlock scalable skill creation and push new capabilities into LLMs with minimal manual effort.\n",
    "\n",
    "You‚Äôre now ready to use these synthetic examples for Fine-tuning small models! \n",
    "\n",
    "Next steps? Try adapting this pipeline to your own task, domain, or format ‚Äî whether it‚Äôs triaging support tickets, extracting structured data, or following domain-specific workflows. The skills are yours to create."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
