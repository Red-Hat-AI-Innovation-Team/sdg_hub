{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InstructLab Skills Synthetic Data Generation\n",
    "\n",
    "![InstructLab Banner](../../../assets/imgs/instructlab-banner.png)\n",
    "\n",
    "This notebook demonstrates how to customize language models by generating training data for specific skills, following the methodology outlined in the LAB (Large-scale Alignment for Chatbots) framework [[paper link](https://arxiv.org/pdf/2403.01081)].\n",
    "\n",
    "### Customizing Model Behavior\n",
    "\n",
    "The LAB framework enables us to shape how a model responds to various tasks by training it on carefully crafted examples. Want your model to write emails in your company's tone? Need it to follow specific formatting guidelines? This customization is achieved through what the paper defines as compositional skills.\n",
    "\n",
    "Compositional skills are tasks that combine different abilities to handle complex queries. For example, if you want your model to write company emails about quarterly performance, it needs to:\n",
    "- Understand financial concepts\n",
    "- Perform basic arithmetic\n",
    "- Write in your preferred communication style\n",
    "- Follow your organization's email format\n",
    "\n",
    "### Demo Overview\n",
    "\n",
    "This notebook will show you how to:\n",
    "1. Set up a teacher model for generating training data\n",
    "2. Create examples that reflect your preferred style and approach\n",
    "3. Generate Synthetic Data\n",
    "4. Validate that the generated data matches your requirements\n",
    "\n",
    "The end goal is to create training data that will help align the model with your specific needs, whether that's matching your company's communication style, following particular protocols, or handling specialized tasks in your preferred way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßë‚Äçüè´ Step 1: Serving Teacher Model\n",
    "\n",
    "We will be using vLLM to serve our models in this demo. But you can host it with your favorite inference engines. All this demo expects is an openai compatible endpoint. \n",
    "\n",
    "For this demo we will use Mixtral-8x7B-Instruct-v0.1 as our teacher model\n",
    "\n",
    "Launch the vLLM server with the following command:\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model=mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "    --dtype=bfloat16 \\\n",
    "    --tensor-parallel-size 2\n",
    "```\n",
    "\n",
    "This will host the model endpoint with default address being `http://localhost:8000`\n",
    "\n",
    "#### Requirements & Considerations\n",
    "- Sufficient GPU memory \n",
    "- Adjust tensor-parallel-size according to available GPUs\n",
    "- Initial model loading may take several minutes\n",
    "\n",
    "#### Let's test the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful! mistralai/Mixtral-8x7B-Instruct-v0.1:  Hello! It's nice to meet you.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "teacher_model = models.data[0].id\n",
    "\n",
    "# Test the connection with a simple completion\n",
    "response = client.chat.completions.create(\n",
    "    model=teacher_model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    temperature=0.0,\n",
    "    max_tokens=10\n",
    ")\n",
    "completion = response.choices[0].message.content\n",
    "\n",
    "print(f\"Connection successful! {teacher_model}: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Step 2: Provide Custom Examples\n",
    "\n",
    "\n",
    "#### Usecase: Teaching a Language Model the Skill: Unstructured Text ‚Üí Markdown Table\n",
    "\n",
    "Company X receives large volumes of user feedback through support emails, in-app surveys, and app store reviews. These messages often contain valuable product insights, but the content is unstructured and difficult to analyze at scale.\n",
    "\n",
    "To streamline internal workflows, an AI team at Company X wants to teach a language model how to convert raw user feedback into structured markdown tables. These tables summarize key topics, user sentiment, and issues in a format that‚Äôs easy to scan, report, or push into dashboards and tracking systems.\n",
    "\n",
    "We can do this using InstructLab!\n",
    "\n",
    "#### üßæ Example Input and Output\n",
    "\n",
    "üì• Input (Unstructured Feedback)\n",
    "```\n",
    "Hey team ‚Äî I‚Äôve been using the new update for about a week now.\n",
    "\n",
    "Couple of things:\n",
    "- The dark mode is awesome, great job!\n",
    "- But the loading time after login feels slower than before. Not a deal breaker but noticeable.\n",
    "- I also noticed that the calendar widget doesn‚Äôt update properly if I change time zones.\n",
    "\n",
    "Overall, I love where this is going. Just needs a few tweaks.\n",
    "```\n",
    "üì§ Output (Markdown Table)\n",
    "\n",
    "| Feature           | Feedback                                                               | Sentiment |\n",
    "|------------------|------------------------------------------------------------------------|-----------|\n",
    "| Dark Mode        | Works well, user is satisfied.                                          | Positive  |\n",
    "| Login Performance| Loading time after login is slower than previous version.               | Negative  |\n",
    "| Calendar Widget  | Doesn't update correctly when time zones change.                        | Negative  |\n",
    "| Overall          | User is happy with the direction of the product, but suggests tweaks.   | Positive  |\n",
    "\n",
    "#### Instructlab Grounded Skills Generation Pipeline \n",
    "\n",
    "Now that we have laid out our usecase, lets dive into the skills generation pipeline proposed by LAB \n",
    "You can refer to the flow details and block config from this yaml (src/instructlab/sdg/flows/generation/skills/simple_grounded_skill.yaml)\n",
    "\n",
    "InstructLab uses a multi-step process of generation and evaluation to generate synthetic data. For grounded skills it looks like this: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <td>\n",
    "    <img src=\"../../../assets/imgs/IL_skills_pipeline.png\" alt=\"Skills Pipeline\" width=\"250\">\n",
    "  </td>\n",
    "  <td>\n",
    "    <ul>\n",
    "      <li>\n",
    "        <strong>Context Generation (<code>gen_contexts</code>)</strong><br>\n",
    "        Generates diverse, relevant contexts for the skill<br>\n",
    "        Produces 10 unique contexts per run<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Question Generation & Validation</strong><br>\n",
    "        <code>gen_grounded_questions</code>: Creates 3 questions per context<br>\n",
    "        <code>eval_grounded_questions</code>: Evaluates question quality<br>\n",
    "        <code>filter_grounded_questions</code>: Keeps only perfect scores (1.0)<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Response Generation & Quality Control</strong><br>\n",
    "        <code>gen_grounded_responses</code>: Generates appropriate responses<br>\n",
    "        <code>evaluate_grounded_qa_pair</code>: Scores Q&A pair quality<br>\n",
    "        <code>filter_grounded_qa_pair</code>: Retains high-quality pairs (score ‚â• 2.0)<br><br>\n",
    "      </li>\n",
    "      <li>\n",
    "        <strong>Final Processing</strong><br>\n",
    "        <code>combine_question_and_context</code>: Merges context with questions for complete examples<br><br>\n",
    "      </li>\n",
    "    </ul>\n",
    "  </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Data with Examples\n",
    "Now that we've seen how LAB generates skill-specific data, let's walk through how to use it for our own use case.\n",
    "\n",
    "As outlined in the LAB paper, the first step is to provide a small number of **seed examples** (typically 5) to bootstrap the skill. These examples are passed into the generation pipeline as input and are stored in a `.jsonl` file.\n",
    "\n",
    "For this demo, we‚Äôll use the pre-populated seed file located at: [mdtable_seeds.jsonl](examples/instructlab/skills/sample_data/mdtable_seeds.jsonl)\n",
    "\n",
    "Lets open the file and explore a row: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 5 examples [00:00, 2975.95 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'task_description': 'Convert the following unstructured user feedback into a structured markdown table.',\n",
       " 'seed_context': \"Been using the new dashboard for a few days. It's way faster than the previous one, really appreciate the snappy filters. But export to CSV seems broken ‚Äî nothing happens when I click it. Also, dark mode resets every time I log in.\",\n",
       " 'seed_question': 'I would like to convert the above feedback into a markdown table with columns for Feature, Feedback and Sentiment.',\n",
       " 'seed_response': \"| Feature           | Feedback                                                           | Sentiment |\\n|------------------|--------------------------------------------------------------------|-----------|\\n| Dashboard        | Much faster than previous version, filters are responsive.         | Positive  |\\n| Export to CSV    | Clicking the export button doesn't trigger a download.             | Negative  |\\n| Dark Mode        | Resets to light mode on login.                                     | Negative  |\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the seed dataset\n",
    "seed_data = load_dataset(\"json\", data_files=\"examples/instructlab/skills/sample_data/mdtable_seeds.jsonl\", split=\"train\")\n",
    "\n",
    "# Display the first example\n",
    "seed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Generate Synthetic Data\n",
    "\n",
    "Now that we have our seed data ready, we can use LAB‚Äôs Skill Data Generator to create **high-quality synthetic training examples** for our custom skill.\n",
    "\n",
    "This step leverages a predefined **flow configuration** that encodes how seed examples are expanded ‚Äî by generating new contexts, questions, and responses, and filtering them for quality.\n",
    "\n",
    "In this demo, we'll use the `synth_grounded_skills.yaml` flow, which follows LAB's grounded generation pattern (context ‚Üí question ‚Üí response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instructlab.sdg.flow import Flow\n",
    "from instructlab.sdg.pipeline import Pipeline\n",
    "from instructlab.sdg.sdg import SDG\n",
    "\n",
    "# Path to the skill generation flow configuration\n",
    "flow_path = \"src/instructlab/sdg/flows/generation/skills/synth_grounded_skills.yaml\"\n",
    "\n",
    "# Load the flow\n",
    "flow = Flow(client).get_flow_from_file(flow_path)\n",
    "\n",
    "# Initialize the synthetic data generator\n",
    "generator = SDG(\n",
    "    [Pipeline(flow)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the generator is ready to run the full pipeline ‚Äî including context generation, question/response generation, evaluation, and filtering ‚Äî to produce a synthetic dataset that can be used for fine-tuning or skill bootstrapping.\n",
    "\n",
    "In the next step, we‚Äôll run this pipeline and inspect the generated outputs. (This should take about a minute or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:22:34] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://0.0.0.0:8000/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>         <a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:22:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://0.0.0.0:8000/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m         \u001b]8;id=200777;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=53993;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:22:44] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://0.0.0.0:8000/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>         <a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:22:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://0.0.0.0:8000/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m         \u001b]8;id=612559;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=201993;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:23:10] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://0.0.0.0:8000/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>         <a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:23:10]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://0.0.0.0:8000/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m         \u001b]8;id=454191;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=895951;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 238/238 [00:00<00:00, 18771.75 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 238/238 [00:00<00:00, 81542.59 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 238/238 [00:00<00:00, 48592.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:23:15] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://0.0.0.0:8000/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>         <a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:23:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://0.0.0.0:8000/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m         \u001b]8;id=891581;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=897602;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:23:19] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://0.0.0.0:8000/v1/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>         <a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:23:19]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://0.0.0.0:8000/v1/completions\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m         \u001b]8;id=416334;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=66756;file:///home/lab/.conda/envs/lab/lib/python3.12/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 15499.86 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 53584.91 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:00<00:00, 34785.75 examples/s]\n",
      "Map (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145/145 [00:00<00:00, 1058.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "generated_data = generator.generate(seed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Explore and Validate the Synthetically Generated Data\n",
    "\n",
    "Once the skill generation pipeline has been executed, the output is a set of **synthetically generated examples** ‚Äî new context-question-response triples that follow the same structure as the seed data but are expanded and refined by the teacher model.\n",
    "\n",
    "Below is an example of one generated entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_description': 'Convert the following unstructured user feedback into a structured markdown table.',\n",
       " 'seed_context': 'The analytics view is very informative. Would love to see breakdown by team as well. Charts sometimes take a few seconds to load though. Mobile layout is clean.',\n",
       " 'seed_question': 'Please convert the above feedback into a markdown table with columns for Feature, Feedback and Sentiment.',\n",
       " 'seed_response': '| Feature           | Feedback                                                             | Sentiment |\\n|------------------|----------------------------------------------------------------------|-----------|\\n| Analytics View    | Provides useful insights.                                           | Positive  |\\n| Team Breakdown    | Requested feature not currently available.                         | Neutral   |\\n| Charts            | Load slowly on occasion.                                            | Negative  |\\n| Mobile Layout     | Clean and well-designed.                                            | Positive  |',\n",
       " 'context': 'The user provided unstructured feedback about the e-learning platform. They mentioned that the video tutorials are helpful and they like the ability to adjust the playback speed. However, they suggested adding a feature to take notes directly on the platform and improve the search functionality for easier navigation to specific topics. The user also expressed that the quizzes are too easy and recommended increasing the difficulty level.',\n",
       " 'question': \"The user provided unstructured feedback about the e-learning platform. They mentioned that the video tutorials are helpful and they like the ability to adjust the playback speed. However, they suggested adding a feature to take notes directly on the platform and improve the search functionality for easier navigation to specific topics. The user also expressed that the quizzes are too easy and recommended increasing the difficulty level.\\n\\nHow can you represent the user's thoughts about the e-learning platform in a markdown table with columns for Feature (video tutorials, playback speed, note-taking feature, search functionality, and quiz difficulty), Feedback, and Sentiment?\",\n",
       " 'response': '| Feature | Feedback | Sentiment |\\n| --- | --- | --- |\\n| Video Tutorials | Found to be helpful. | Positive |\\n| Playback Speed | User likes the ability to adjust. | Positive |\\n| Note-taking Feature | Requested feature for taking notes directly on the platform. | Neutral |\\n| Search Functionality | Suggested to improve for easier navigation to specific topics. | Neutral |\\n| Quiz Difficulty | User finds the quizzes too easy, recommended increasing the difficulty level. | Negative |',\n",
       " 'evaluation': \"This answer is a good example of how an AI Assistant should respond to the user's instruction. The answer provides a markdown table with columns for Feature, Feedback, and Sentiment, and each row in the table represents a specific aspect of the e-learning platform mentioned in the user's feedback. The answer is safe, easy to follow, and demonstrates expert knowledge in creating markdown tables.\",\n",
       " 'score': 3.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "rand_idx = random.choice(range(len(generated_data)))\n",
    "generated_data[rand_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user provided unstructured feedback about the e-learning platform. They mentioned that the video tutorials are helpful and they like the ability to adjust the playback speed. However, they suggested adding a feature to take notes directly on the platform and improve the search functionality for easier navigation to specific topics. The user also expressed that the quizzes are too easy and recommended increasing the difficulty level.\n",
      "\n",
      "How can you represent the user's thoughts about the e-learning platform in a markdown table with columns for Feature (video tutorials, playback speed, note-taking feature, search functionality, and quiz difficulty), Feedback, and Sentiment?\n"
     ]
    }
   ],
   "source": [
    "print(generated_data[rand_idx]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Feature | Feedback | Sentiment |\n",
      "| --- | --- | --- |\n",
      "| Video Tutorials | Found to be helpful. | Positive |\n",
      "| Playback Speed | User likes the ability to adjust. | Positive |\n",
      "| Note-taking Feature | Requested feature for taking notes directly on the platform. | Neutral |\n",
      "| Search Functionality | Suggested to improve for easier navigation to specific topics. | Neutral |\n",
      "| Quiz Difficulty | User finds the quizzes too easy, recommended increasing the difficulty level. | Negative |\n"
     ]
    }
   ],
   "source": [
    "print(generated_data[rand_idx]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to teach a custom skill to a language model using the InstructLab Skill Data Generator (SDG). Starting from a small set of seed examples, we walked through the full synthetic data generation pipeline ‚Äî including context creation, question generation, response synthesis, evaluation, and filtering.\n",
    "\n",
    "We explored a real-world use case: **transforming unstructured user feedback into structured markdown tables**, and showed how the LAB framework can automate the generation of high-quality, instructional training data at scale.\n",
    "\n",
    "This approach is especially powerful for procedural or domain-specific tasks where labeled data is scarce but consistent task logic can be modeled. With just a few carefully curated seed examples, you can unlock scalable skill creation and push new capabilities into LLMs with minimal manual effort.\n",
    "\n",
    "You‚Äôre now ready to use these synthetic examples for Fine-tuning small models! \n",
    "\n",
    "Next steps? Try adapting this pipeline to your own task, domain, or format ‚Äî whether it‚Äôs triaging support tickets, extracting structured data, or following domain-specific workflows. The skills are yours to create."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
